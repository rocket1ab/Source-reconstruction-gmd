{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import graphviz\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "# 下面进行特征筛选\n",
    "from sklearn.feature_selection import RFECV\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "import  pandas  as  pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold,train_test_split,GridSearchCV, cross_val_score\n",
    "#from clean_data import prep_water_data, normalize_water_data, normalize_data, delete_null_date\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score,train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import plot_tree\n",
    "from time import *\n",
    "import os\n",
    "import cmath as ch\n",
    "import scipy.io\n",
    "import heapq\n",
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "begin_time = time() # 统计训练时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = './datafile.mat'\n",
    "data_info = scipy.io.loadmat(source_data)\n",
    "data = data_info['obs_data']\n",
    "sim_data = data_info['sim_data']\n",
    "Sample_amount = sim_data.shape[0]\n",
    "Scaling_factor = data_info['Scaling_factor']\n",
    "release_loc = data_info['release_loc']\n",
    "\n",
    "start_time = 0\n",
    "end_time = 15\n",
    "interval = end_time - start_time\n",
    "percentile = 0.4\n",
    "J = []\n",
    "measure1 = 0\n",
    "measure2 = 0\n",
    "measure3 = 0\n",
    "measure4 = 0\n",
    "measure = []\n",
    "for k in range(start_time,end_time):\n",
    "    measure1 = measure1+data[k,0]\n",
    "    measure2 = measure2+data[k,1]\n",
    "    measure3 = measure3+data[k,2]\n",
    "    measure4 = measure4+data[k,3]\n",
    "measure_aver1 = measure1 / interval #测点1测量值的平均值\n",
    "measure.append(measure_aver1)\n",
    "measure_aver2 = measure2 / interval #测点2测量值的平均值\n",
    "measure.append(measure_aver2)\n",
    "measure_aver3 = measure3 / interval #测点3测量值的平均值\n",
    "measure.append(measure_aver3)\n",
    "measure_aver4 = measure4 / interval #测点4测量值的平均值\n",
    "measure.append(measure_aver4)\n",
    "measure_aver = (measure_aver1+measure_aver2+measure_aver3+measure_aver4)/4\n",
    "\n",
    "#读取释放点的模拟数据\n",
    "for j in range(Sample_amount):\n",
    "    Oct4_sim = [\n",
    "                    np.mean(sim_data[j,start_time:end_time,0]),\n",
    "                    np.mean(sim_data[j,start_time:end_time,1]),\n",
    "                    np.mean(sim_data[j,start_time:end_time,2]),\n",
    "                    np.mean(sim_data[j,start_time:end_time,3])\n",
    "               ]\n",
    "    \n",
    "    Oct4_sim_aver = np.mean(Oct4_sim)\n",
    "    sum1_1=0\n",
    "    sum1_2=0\n",
    "    sum1_3=0\n",
    "    for i in range(0,4):\n",
    "        measure_bias = measure[i] - measure_aver\n",
    "        measure_var = measure_bias ** 2\n",
    "        Oct4_sim_bias = Oct4_sim[i] - Oct4_sim_aver\n",
    "        Oct4_sim_var = Oct4_sim_bias ** 2\n",
    "        temp1_1 = measure_bias * Oct4_sim_bias\n",
    "        sum1_1 = sum1_1 + temp1_1\n",
    "        sum1_2 = sum1_2 + Oct4_sim_var\n",
    "        sum1_3 = sum1_3 + measure_var\n",
    "    aver1_1 = sum1_1/4\n",
    "    aver1_2 = np.sqrt(sum1_2/4)\n",
    "    aver1_3 = np.sqrt(sum1_3/4) \n",
    "    J1_temp = aver1_1/(aver1_2*aver1_3)\n",
    "\n",
    "    if np.isnan(J1_temp):\n",
    "        J1_temp = -1 + np.random.uniform(0,0.1)\n",
    "    J.append(J1_temp)\n",
    "\n",
    "index = J.index(max(J))\n",
    "\n",
    "def OperCount(data,number,operator):\n",
    "    num = 0\n",
    "    if operator == '>':\n",
    "        for i in data:\n",
    "            if i > number:\n",
    "                num+=1\n",
    "    elif operator == '<':\n",
    "        for i in data:\n",
    "            if i < number:\n",
    "                num+=1\n",
    "    elif operator == '=':\n",
    "        for i in data:\n",
    "            if i == number:\n",
    "                num+=1\n",
    "    else:\n",
    "        print('something wrong!')\n",
    "    percent = num/len(data)\n",
    "    return num,percent\n",
    "\n",
    "J = list(J)\n",
    "index_train = map(J.index,heapq.nlargest(int(Sample_amount*percentile),J))\n",
    "index_train = list(index_train)\n",
    "\n",
    "J = pd.DataFrame(J)\n",
    "writer = pd.ExcelWriter('./Reconstruction_results/cost_function_Group2.xls')\n",
    "J.to_excel(writer,header=False,index=False)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = np.array(index_train)\n",
    "index_train = index_train.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Training_datasets/train_data.csv\",index_col=0) # 读取特征数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[index_train,:]\n",
    "data.info()\n",
    "data\n",
    "# 将数据中inf值都设置为0\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature = data.drop([\"x\",\"y\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target = data.iloc[:,24:26]\n",
    "data_target_x = data.iloc[:,24:25]\n",
    "data_target_y = data.iloc[:,25:26]\n",
    "data_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(data_feature,data_target,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain, Xtest, Ytrain, Ytest]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    "\n",
    "Xtrain_x, Xtest_x, Ytrain_x, Ytest_x = train_test_split(data_feature,data_target_x,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_x, Xtest_x, Ytrain_x, Ytest_x]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    " \n",
    "Xtrain_y, Xtest_y, Ytrain_y, Ytest_y = train_test_split(data_feature,data_target_y,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_y, Xtest_y, Ytrain_y, Ytest_y]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MCVA = []\n",
    "MCVA_x = []\n",
    "MCVA_y = []\n",
    "\n",
    "GC = []\n",
    "GC_x = []\n",
    "GC_y = []\n",
    "TS = []\n",
    "LE = []\n",
    "x_estimate = []\n",
    "y_estimate = []\n",
    "n_estimators = []\n",
    "learning_rate = []\n",
    "max_depth = []\n",
    "gamma = []\n",
    "subsample = []\n",
    "reg_lambda = []\n",
    "colsample_bytree = []\n",
    "min_child_weight = []\n",
    "\n",
    "def geodistance(lng1,lat1,lng2,lat2):\n",
    "#lng1,lat1,lng2,lat2 = (120.12802999999997,30.28708,115.86572000000001,28.7427)\n",
    "    lng1, lat1, lng2, lat2 = map(radians, [float(lng1), float(lat1), float(lng2), float(lat2)])\n",
    "# 经纬度转换成弧度\n",
    "    dlon=lng2-lng1\n",
    "    dlat=lat2-lat1\n",
    "    a=sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    distance=2*asin(sqrt(a))*6371*1000 # 地球平均半径，6371km\n",
    "    distance=round(distance/1000,3)\n",
    "    return distance\n",
    "\n",
    "for i in range(50):\n",
    "    print('Loop time：',i)\n",
    "    def huber_approx_obj(real,predict):\n",
    "        d = predict -real\n",
    "        h = 2 #h is delta in the formula\n",
    "        scale = 1 + (d / h) ** 2\n",
    "        scale_sqrt = np.sqrt(scale)\n",
    "        grad = d / scale_sqrt\n",
    "        hess = 1 / scale / scale_sqrt\n",
    "        return grad, hess\n",
    "\n",
    "    space={ 'n_estimators': hp.uniform ('n_estimators', 50,300),\n",
    "            'learning_rate': hp.uniform('learning_rate', 0.05, 0.3),\n",
    "            'max_depth': hp.quniform(\"max_depth\", 3, 8, 1),\n",
    "            'gamma': hp.uniform('gamma', 0.01,1),\n",
    "            'subsample' : hp.uniform('subsample', 0.5,1),\n",
    "            'reg_lambda' : hp.uniform('reg_lambda', 0.01,5),\n",
    "            'colsample_bytree' : hp.uniform('colsample_bytree', 0.01,1),\n",
    "            'min_child_weight' : hp.quniform('min_child_weight', 2, 10, 1),   \n",
    "        }\n",
    "\n",
    "    def hyperparameter_tuning(space):\n",
    "        param = {\n",
    "                 'n_estimators':     int(space['n_estimators']), \n",
    "                 'learning_rate':    space['learning_rate'], \n",
    "                 'max_depth':        int(space['max_depth']),\n",
    "                 'gamma':            space['gamma'], \n",
    "                 'subsample':        space['subsample'],\n",
    "                 'reg_lambda':       space['reg_lambda'],\n",
    "                 'colsample_bytree': space['colsample_bytree'],\n",
    "                 'min_child_weight': space['min_child_weight'],  \n",
    "                 \"random_state\":123456\n",
    "                }\n",
    "        model = MultiOutputRegressor(XGBR(objective='reg:squarederror',**param)) #objective='reg:squarederror'\n",
    "        r2 = cross_val_score(model,data_feature,data_target,cv=5) #scoring='neg_mean_squared_error'\n",
    "        return (1-r2.mean())**2+r2.var()\n",
    "    from hyperopt import tpe\n",
    "    tpe_algorithm = tpe.suggest\n",
    "    from hyperopt import fmin\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=hyperparameter_tuning,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=100,\n",
    "                trials=trials)\n",
    "\n",
    "\n",
    "    print (best)\n",
    "    end_time = time()\n",
    "\n",
    "    run_time = end_time-begin_time\n",
    "\n",
    "    print(run_time)\n",
    "    \n",
    "    learning_rate.append(best['learning_rate'])\n",
    "    max_depth.append(int(best['max_depth']))\n",
    "    n_estimators.append(int(best['n_estimators']))\n",
    "    min_child_weight.append(best['min_child_weight'])\n",
    "    subsample.append(best['subsample'])\n",
    "    colsample_bytree.append(best['colsample_bytree'])\n",
    "    reg_lambda.append(best['reg_lambda'])\n",
    "    gamma.append(best['gamma'])\n",
    "    \n",
    "    param = {\n",
    "                 'max_depth':        int(best['max_depth']),\n",
    "                 'learning_rate':    best['learning_rate'],\n",
    "                 'n_estimators':     int(best['n_estimators']),  \n",
    "                 'min_child_weight': best['min_child_weight'],\n",
    "                 'subsample':        best['subsample'],\n",
    "                 'colsample_bytree': best['colsample_bytree'],\n",
    "                 'reg_lambda':       best['reg_lambda'],\n",
    "                 'gamma':            best['gamma'], \n",
    "                 \"random_state\":123456\n",
    "                }\n",
    "    x_xgbr = XGBR(objective='reg:squarederror',**param)\n",
    "    scores_x = cross_val_score(x_xgbr,data_feature,data_target_x,cv=5)\n",
    "    MCVA_x_temp = scores_x.mean()\n",
    "    MCVA_x_temp = np.around(MCVA_x_temp, decimals = 4)\n",
    "    MCVA_x.append(MCVA_x_temp)\n",
    "    \n",
    "    GC_x_temp = (1-scores_x.mean())**2+scores_x.var()\n",
    "    GC_x_temp = np.around(GC_x_temp, decimals = 4)\n",
    "    GC_x.append(GC_x_temp)\n",
    "    \n",
    "    x_xgbr.fit(Xtrain_x,Ytrain_x)\n",
    "\n",
    "    y_xgbr = XGBR(objective='reg:squarederror',**param)\n",
    "    scores_y = cross_val_score(y_xgbr,data_feature,data_target_y,cv=5)\n",
    "    MCVA_y_temp = scores_y.mean()\n",
    "    MCVA_y_temp = np.around(MCVA_y_temp, decimals = 4)\n",
    "    MCVA_y.append(MCVA_y_temp)\n",
    "    \n",
    "    GC_y_temp = (1-scores_y.mean())**2+scores_y.var()\n",
    "    GC_y_temp = np.around(GC_y_temp, decimals = 4)\n",
    "    GC_y.append(GC_y_temp)\n",
    "    \n",
    "    y_xgbr.fit(Xtrain_y,Ytrain_y)\n",
    "\n",
    "    multioutputregressor = MultiOutputRegressor(XGBR(objective='reg:squarederror',**param))\n",
    "    scores = cross_val_score(multioutputregressor,data_feature,data_target,cv=5)\n",
    "    MCVA_temp = scores.mean()\n",
    "    MCVA_temp = np.around(MCVA_temp, decimals = 4)\n",
    "    MCVA.append(MCVA_temp)\n",
    "    \n",
    "    GC_temp = (1-scores.mean())**2+scores.var()\n",
    "    GC_temp = np.around(GC_temp, decimals = 4)\n",
    "    GC.append(GC_temp)\n",
    "    \n",
    "    multioutputregressor.fit(Xtrain, Ytrain)\n",
    "\n",
    "    print('Prediction result: x, y')\n",
    "    TS_temp = multioutputregressor.score(Xtest,Ytest)\n",
    "    TS_temp = np.around(TS_temp, decimals = 4)\n",
    "    TS.append(TS_temp)\n",
    "    \n",
    "    source = pd.read_csv(\"./Testing_datasets/test_data.csv\",index_col=0) \n",
    "\n",
    "\n",
    "    Multi_pred_source=multioutputregressor.predict(source)\n",
    "    print(Multi_pred_source)\n",
    "    Multi_pred_source_x=Multi_pred_source[0,0]\n",
    "    Multi_pred_source_x = np.around(Multi_pred_source_x,decimals=4)\n",
    "    x_estimate.append(Multi_pred_source_x)\n",
    "    Multi_pred_source_y=Multi_pred_source[0,1]\n",
    "    Multi_pred_source_y = np.around(Multi_pred_source_y,decimals=4)\n",
    "    y_estimate.append(Multi_pred_source_y)\n",
    "    LE_temp = geodistance(-2.0083,48.058,Multi_pred_source_x,Multi_pred_source_y)\n",
    "    LE_temp = np.around(LE_temp, decimals = 4)\n",
    "    LE.append(LE_temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inversion_results = {'MCVA':MCVA,\n",
    "                    'MCVA_x':MCVA_x,\n",
    "                    'MCVA_y':MCVA_y,\n",
    "                    'GC':GC,\n",
    "                    'GC_x':GC_x,\n",
    "                    'GC_y':GC_y,\n",
    "                    'TS': TS,\n",
    "                    'x':x_estimate,\n",
    "                    'y':y_estimate,\n",
    "                    'LE':LE,\n",
    "                    'max_depth':max_depth,\n",
    "                    'learning_rate':learning_rate,\n",
    "                    'n_estimators':n_estimators,\n",
    "                    'min_child_weight':min_child_weight,\n",
    "                    'subsample':subsample,\n",
    "                    'colsample_bytree':colsample_bytree,\n",
    "                    'reg_lambda':reg_lambda,\n",
    "                    'gamma':gamma\n",
    "                    }\n",
    "Inversion_results['x'] = np.around(Inversion_results['x'],decimals=4)\n",
    "Inversion_results['y'] = np.around(Inversion_results['y'],decimals=4)\n",
    "Inversion_results = pd.DataFrame(Inversion_results)\n",
    "Inversion_results.to_csv('./Reconstruction_results/inversion_Group2_before_feature_selection_50.csv')\n",
    "\n",
    "# 找出定位误差最小的一组结果对应的参数\n",
    "best = {'MCVA':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['MCVA'].values,\n",
    "                    'MCVA_x':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['MCVA_x'].values,\n",
    "                    'MCVA_y':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['MCVA_y'].values,\n",
    "                    'GC':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['GC'].values,\n",
    "                    'GC_x':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['GC_x'].values,\n",
    "                    'GC_y':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['GC_y'].values,\n",
    "                    'TS': Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['TS'].values,\n",
    "                    'x':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['x'].values,\n",
    "                    'y':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['y'].values,\n",
    "                    'LE':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['LE'].values,\n",
    "                    'max_depth':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['max_depth'].values,\n",
    "                    'learning_rate':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['learning_rate'].values,\n",
    "                    'n_estimators':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['n_estimators'].values,\n",
    "                    'min_child_weight':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['min_child_weight'].values,\n",
    "                    'subsample':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['subsample'].values,\n",
    "                    'colsample_bytree':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['colsample_bytree'].values,\n",
    "                    'reg_lambda':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['reg_lambda'].values,\n",
    "                    'gamma':Inversion_results[Inversion_results['LE']==np.min(Inversion_results['LE'].values)]['gamma'].values,\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtrain_x, Xtest_x, Ytrain_x, Ytest_x = train_test_split(data_feature,data_target_x,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_x, Xtest_x, Ytrain_x, Ytest_x]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    "# 查看分好的训练集和测试集\n",
    "\n",
    "Xtrain_y, Xtest_y, Ytrain_y, Ytest_y = train_test_split(data_feature,data_target_y,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_y, Xtest_y, Ytrain_y, Ytest_y]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    "#Xtrain.head()\n",
    "\n",
    "other_params = {\n",
    "                 'max_depth':        int(best['max_depth'][0]),\n",
    "                 'learning_rate':    best['learning_rate'][0],\n",
    "                 'n_estimators':     int(best['n_estimators'][0]),  \n",
    "                 'min_child_weight': best['min_child_weight'][0],\n",
    "                 'subsample':        best['subsample'][0],\n",
    "                 'colsample_bytree': best['colsample_bytree'][0],\n",
    "                 'reg_lambda':       best['reg_lambda'][0],\n",
    "                 'gamma':            best['gamma'][0], \n",
    "                 \"random_state\":123456\n",
    "}\n",
    "\n",
    "x_xgbr = XGBR(objective='reg:squarederror',**other_params)\n",
    "y_xgbr = XGBR(objective='reg:squarederror',**other_params)\n",
    "\n",
    "\n",
    "selectorx = RFECV(x_xgbr,cv=5,step=1).fit(Xtrain_x,Ytrain_x)\n",
    "selectory = RFECV(y_xgbr,cv=5,step=1).fit(Xtrain_y,Ytrain_y)\n",
    "#check=multioutputregressor.predict(Xtest)\n",
    "#multioutputregressor.score(Xtest,Ytest)#默认为Return the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "# 绘制x,y随特征数量变化的交叉验证得分\n",
    "# 绘制主图\n",
    "fig,axes=plt.subplots(1,2,figsize=(12,4))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.rc('font',family='Times New Roman',size=16)\n",
    "axes[0].plot(range(1,len(selectorx.cv_results_['mean_test_score'])+1),selectorx.cv_results_['mean_test_score'],label='x',linewidth=3,color='b')\n",
    "axes[0].plot(selectorx.support_.sum(),selectorx.cv_results_['mean_test_score'][selectorx.support_.sum()-1],marker='*', color='black', markersize=12,alpha=0.7)\n",
    "axes[0].set_xlabel('Number of features',fontproperties='Times New Roman',fontsize=16,weight='bold')\n",
    "axes[0].set_ylabel('MCV',fontproperties='Times New Roman',fontsize=16,weight='bold')\n",
    "axes[0].set_title('Feature selection by x',fontproperties='Times New Roman',fontsize=20,weight='bold')\n",
    "legend = axes[0].legend(edgecolor = 'black',loc=4)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontweight('bold')  # 加粗\n",
    "axes[0].tick_params(axis='y',labelsize=16)\n",
    "axes[0].tick_params(axis='x',labelsize=16)\n",
    "labels = axes[0].get_xticklabels()+axes[0].get_yticklabels()\n",
    "[label.set_fontname('Times New Roman') for label in labels]\n",
    "[label.set_weight('bold') for label in labels]  # 加粗\n",
    "\n",
    "axes[1].plot(range(1,len(selectory.cv_results_['mean_test_score'])+1),selectory.cv_results_['mean_test_score'],label='y',linewidth=3,color='r')\n",
    "axes[1].plot(selectory.support_.sum(),selectory.cv_results_['mean_test_score'][selectory.support_.sum()-1],marker='*', color='black', markersize=12,alpha=0.7)\n",
    "axes[1].set_xlabel('Number of features',fontproperties='Times New Roman',fontsize=16,weight='bold')\n",
    "axes[1].set_ylabel('MCV',fontproperties='Times New Roman',fontsize=16,weight='bold')\n",
    "axes[1].set_title('Feature selection by y',fontproperties='Times New Roman',fontsize=20,weight='bold')\n",
    "legend = axes[1].legend(edgecolor = 'black',loc=4)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontweight('bold')  # 加粗\n",
    "axes[1].tick_params(axis='y',labelsize=16)\n",
    "axes[1].tick_params(axis='x',labelsize=16)\n",
    "labels = axes[1].get_xticklabels()+axes[1].get_yticklabels()\n",
    "[label.set_fontname('Times New Roman') for label in labels]\n",
    "[label.set_weight('bold') for label in labels]  # 加粗\n",
    "\n",
    "fig.savefig(\"./Reconstruction_results/Feature_selection_Group2.png\",dpi=900,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(selectorx.support_.sum())\n",
    "print(selectorx.ranking_)\n",
    "idx=[]\n",
    "for i in range(24):\n",
    "    if selectorx.ranking_[i]!=1:\n",
    "        idx.append(i)\n",
    "print(idx)\n",
    "X_wrapper_x=selectorx.transform(Xtrain_x)\n",
    "cross_val_score(x_xgbr,X_wrapper_x,Ytrain_x,cv=5).mean()\n",
    "\n",
    "print(selectory.support_.sum())\n",
    "print(selectory.ranking_)\n",
    "idy=[]\n",
    "for i in range(24):\n",
    "    if selectory.ranking_[i]!=1:\n",
    "        idy.append(i)\n",
    "print(idy)\n",
    "X_wrapper_y=selectory.transform(Xtrain_y)\n",
    "cross_val_score(y_xgbr,X_wrapper_y,Ytrain_y,cv=5).mean()\n",
    "\n",
    "\n",
    "id_delete = list(set(idx).intersection(set(idy)))\n",
    "\n",
    "data_feature_all = ['wave_rate1','aver1','median1','fft_shape_mean1','fft_mean1','SamEn1',\n",
    "                   'wave_rate2','aver2','median2','fft_shape_mean2','fft_mean2','SamEn2',\n",
    "                   'wave_rate3','aver3','median3','fft_shape_mean3','fft_mean3','SamEn3',\n",
    "                   'wave_rate4','aver4','median4','fft_shape_mean4','fft_mean4','SamEn4',]\n",
    "data_feature = data.drop([\"x\",\"y\"],axis=1)\n",
    "for i in range(len(id_delete)):\n",
    "    data_feature = data_feature.drop(data_feature_all[id_delete[i]],axis=1)\n",
    "print(data_feature)\n",
    "\n",
    "data_feature.replace([np.inf,-np.inf],np.nan,inplace=True)\n",
    "data_feature = data_feature.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(data_feature,data_target,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain, Xtest, Ytrain, Ytest]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    "\n",
    "Xtrain_x, Xtest_x, Ytrain_x, Ytest_x = train_test_split(data_feature,data_target_x,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_x, Xtest_x, Ytrain_x, Ytest_x]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    " \n",
    "Xtrain_y, Xtest_y, Ytrain_y, Ytest_y = train_test_split(data_feature,data_target_y,test_size=0.3,random_state=320)\n",
    "for i in [Xtrain_y, Xtest_y, Ytrain_y, Ytest_y]:\n",
    "    i.index = range(i.shape[0]) #将索引转化成0-shape[0]的整数，就可以恢复索引\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCVA = []\n",
    "MCVA_x = []\n",
    "MCVA_y = []\n",
    "\n",
    "GC = []\n",
    "GC_x = []\n",
    "GC_y = []\n",
    "TS = []\n",
    "LE = []\n",
    "x_estimate = []\n",
    "y_estimate = []\n",
    "x_estimate = []\n",
    "y_estimate = []\n",
    "n_estimators = []\n",
    "learning_rate = []\n",
    "max_depth = []\n",
    "gamma = []\n",
    "subsample = []\n",
    "reg_lambda = []\n",
    "colsample_bytree = []\n",
    "min_child_weight = []\n",
    "\n",
    "for i in range(50):\n",
    "    print('Loop time：',i)\n",
    "    def huber_approx_obj(real,predict):\n",
    "        d = predict -real\n",
    "        h = 2 #h is delta in the formula\n",
    "        scale = 1 + (d / h) ** 2\n",
    "        scale_sqrt = np.sqrt(scale)\n",
    "        grad = d / scale_sqrt\n",
    "        hess = 1 / scale / scale_sqrt\n",
    "        return grad, hess\n",
    "\n",
    "    space={ 'n_estimators': hp.uniform ('n_estimators', 50,300),\n",
    "            'learning_rate': hp.uniform('learning_rate', 0.05, 0.3),\n",
    "            'max_depth': hp.quniform(\"max_depth\", 3, 8, 1),\n",
    "            'gamma': hp.uniform('gamma', 0.01,1),\n",
    "            'subsample' : hp.uniform('subsample', 0.5,1),\n",
    "            'reg_lambda' : hp.uniform('reg_lambda', 0.01,5),\n",
    "            'colsample_bytree' : hp.uniform('colsample_bytree', 0.01,1),\n",
    "            'min_child_weight' : hp.quniform('min_child_weight', 2, 10, 1),   \n",
    "        }\n",
    "\n",
    "    def hyperparameter_tuning(space):\n",
    "        param = {\n",
    "                 'n_estimators':     int(space['n_estimators']), \n",
    "                 'learning_rate':    space['learning_rate'], \n",
    "                 'max_depth':        int(space['max_depth']),\n",
    "                 'gamma':            space['gamma'], \n",
    "                 'subsample':        space['subsample'],\n",
    "                 'reg_lambda':       space['reg_lambda'],\n",
    "                 'colsample_bytree': space['colsample_bytree'],\n",
    "                 'min_child_weight': space['min_child_weight'],  \n",
    "                 \"random_state\":123456\n",
    "                }\n",
    "        model = MultiOutputRegressor(XGBR(objective='reg:squarederror',**param)) #objective='reg:squarederror'\n",
    "        r2 = cross_val_score(model,data_feature,data_target,cv=5) #scoring='neg_mean_squared_error'\n",
    "        return (1-r2.mean())**2+r2.var()\n",
    "\n",
    "    from hyperopt import tpe\n",
    "    tpe_algorithm = tpe.suggest\n",
    "    from hyperopt import fmin\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=hyperparameter_tuning,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=100,\n",
    "                trials=trials)\n",
    "\n",
    "\n",
    "    print (best)\n",
    "    end_time = time()\n",
    "\n",
    "    run_time = end_time-begin_time\n",
    "\n",
    "    print(run_time)\n",
    "    \n",
    "    learning_rate.append(best['learning_rate'])\n",
    "    max_depth.append(int(best['max_depth']))\n",
    "    n_estimators.append(int(best['n_estimators']))\n",
    "    min_child_weight.append(best['min_child_weight'])\n",
    "    subsample.append(best['subsample'])\n",
    "    colsample_bytree.append(best['colsample_bytree'])\n",
    "    reg_lambda.append(best['reg_lambda'])\n",
    "    gamma.append(best['gamma'])\n",
    "    \n",
    "    param = {\n",
    "                 'max_depth':        int(best['max_depth']),\n",
    "                 'learning_rate':    best['learning_rate'],\n",
    "                 'n_estimators':     int(best['n_estimators']),  \n",
    "                 'min_child_weight': best['min_child_weight'],\n",
    "                 'subsample':        best['subsample'],\n",
    "                 'colsample_bytree': best['colsample_bytree'],\n",
    "                 'reg_lambda':       best['reg_lambda'],\n",
    "                 'gamma':            best['gamma'], \n",
    "                 \"random_state\":123456\n",
    "                }\n",
    "    x_xgbr = XGBR(objective='reg:squarederror',**param)\n",
    "    scores_x = cross_val_score(x_xgbr,data_feature,data_target_x,cv=5)\n",
    "    MCVA_x_temp = scores_x.mean()\n",
    "    MCVA_x_temp = np.around(MCVA_x_temp, decimals = 4)\n",
    "    MCVA_x.append(MCVA_x_temp)\n",
    "    \n",
    "    GC_x_temp = (1-scores_x.mean())**2+scores_x.var()\n",
    "    GC_x_temp = np.around(GC_x_temp, decimals = 4)\n",
    "    GC_x.append(GC_x_temp)\n",
    "    \n",
    "    x_xgbr.fit(Xtrain_x,Ytrain_x)\n",
    "\n",
    "    y_xgbr = XGBR(objective='reg:squarederror',**param)\n",
    "    scores_y = cross_val_score(y_xgbr,data_feature,data_target_y,cv=5)\n",
    "    MCVA_y_temp = scores_y.mean()\n",
    "    MCVA_y_temp = np.around(MCVA_y_temp, decimals = 4)\n",
    "    MCVA_y.append(MCVA_y_temp)\n",
    "    \n",
    "    GC_y_temp = (1-scores_y.mean())**2+scores_y.var()\n",
    "    GC_y_temp = np.around(GC_y_temp, decimals = 4)\n",
    "    GC_y.append(GC_y_temp)\n",
    "    \n",
    "    y_xgbr.fit(Xtrain_y,Ytrain_y)\n",
    "\n",
    "    multioutputregressor = MultiOutputRegressor(XGBR(objective='reg:squarederror',**param))\n",
    "    scores = cross_val_score(multioutputregressor,data_feature,data_target,cv=5)\n",
    "    MCVA_temp = scores.mean()\n",
    "    MCVA_temp = np.around(MCVA_temp, decimals = 4)\n",
    "    MCVA.append(MCVA_temp)\n",
    "    \n",
    "    GC_temp = (1-scores.mean())**2+scores.var()\n",
    "    GC_temp = np.around(GC_temp, decimals = 4)\n",
    "    GC.append(GC_temp)\n",
    "    \n",
    "    multioutputregressor.fit(Xtrain, Ytrain)\n",
    "\n",
    "    TS_temp = multioutputregressor.score(Xtest,Ytest)\n",
    "    TS_temp = np.around(TS_temp, decimals = 4)\n",
    "    TS.append(TS_temp)\n",
    "    # 对测点的数据进行验证\n",
    "    source_feature_all = ['wave_rate1','aver1','median1','fft_shape_mean1','fft_mean1','SamEn1',\n",
    "                   'wave_rate2','aver2','median2','fft_shape_mean2','fft_mean2','SamEn2',\n",
    "                   'wave_rate3','aver3','median3','fft_shape_mean3','fft_mean3','SamEn3',\n",
    "                   'wave_rate4','aver4','median4','fft_shape_mean4','fft_mean4','SamEn4',]\n",
    "    source = pd.read_csv(\"./Testing_datasets/test_data.csv\",index_col=0)\n",
    "    for i in range(len(id_delete)):\n",
    "        source = source.drop(source_feature_all[id_delete[i]],axis=1)\n",
    "\n",
    "    Multi_pred_source=multioutputregressor.predict(source)\n",
    "    print(Multi_pred_source)\n",
    "    Multi_pred_source_x=Multi_pred_source[0,0]\n",
    "    Multi_pred_source_x = np.around(Multi_pred_source_x,decimals=4)\n",
    "    x_estimate.append(Multi_pred_source_x)\n",
    "    Multi_pred_source_y=Multi_pred_source[0,1]\n",
    "    Multi_pred_source_y = np.around(Multi_pred_source_y,decimals=4)\n",
    "    y_estimate.append(Multi_pred_source_y)\n",
    "    LE_temp = geodistance(-2.0083,48.058,Multi_pred_source_x,Multi_pred_source_y)\n",
    "    LE_temp = np.around(LE_temp, decimals = 4)\n",
    "    LE.append(LE_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inversion_results = {'MCVA':MCVA,\n",
    "                    'MCVA_x':MCVA_x,\n",
    "                    'MCVA_y':MCVA_y,\n",
    "                    'GC':GC,\n",
    "                    'GC_x':GC_x,\n",
    "                    'GC_y':GC_y,\n",
    "                    'TS': TS,\n",
    "                    'x':x_estimate,\n",
    "                    'y':y_estimate,\n",
    "                    'LE':LE,\n",
    "                    'max_depth':max_depth,\n",
    "                    'learning_rate':learning_rate,\n",
    "                    'n_estimators':n_estimators,\n",
    "                    'min_child_weight':min_child_weight,\n",
    "                    'subsample':subsample,\n",
    "                    'colsample_bytree':colsample_bytree,\n",
    "                    'reg_lambda':reg_lambda,\n",
    "                    'gamma':gamma\n",
    "                    }\n",
    "Inversion_results['x'] = np.around(Inversion_results['x'],decimals=4)\n",
    "Inversion_results['y'] = np.around(Inversion_results['y'],decimals=4)\n",
    "Inversion_results = pd.DataFrame(Inversion_results)\n",
    "Inversion_results.to_csv('./Reconstruction_results/inversion_Group2_after_feature_selection_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 置信区间统计\n",
    "inversion_before_FS  = pd.read_csv('./Reconstruction_results/inversion_Group2_before_feature_selection_50.csv',index_col=0)\n",
    "inversion_before_FS_x_aver = np.mean(inversion_before_FS['x'].values)\n",
    "inversion_before_FS_x_std = np.std(inversion_before_FS['x'].values)\n",
    "inversion_before_FS_x_low = inversion_before_FS_x_aver - 1.96 * inversion_before_FS_x_std\n",
    "inversion_before_FS_x_high = inversion_before_FS_x_aver + 1.96 * inversion_before_FS_x_std\n",
    "\n",
    "inversion_before_FS_y_aver = np.mean(inversion_before_FS['y'].values)\n",
    "inversion_before_FS_y_std = np.std(inversion_before_FS['y'].values)\n",
    "inversion_before_FS_y_low = inversion_before_FS_y_aver - 1.96 * inversion_before_FS_y_std\n",
    "inversion_before_FS_y_high = inversion_before_FS_y_aver + 1.96 * inversion_before_FS_y_std\n",
    "\n",
    "print(\"Before feature selection\")\n",
    "print(\"x interval: [\", inversion_before_FS_x_low, \",\" ,inversion_before_FS_x_high, \"]\")\n",
    "print(\"y interval: [\", inversion_before_FS_y_low, \",\" ,inversion_before_FS_y_high, \"]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_after_FS  = pd.read_csv('./Reconstruction_results/inversion_Group2_after_feature_selection_50.csv',index_col=0)\n",
    "inversion_after_FS_x_aver = np.mean(inversion_after_FS['x'].values)\n",
    "inversion_after_FS_x_std = np.std(inversion_after_FS['x'].values)\n",
    "inversion_after_FS_x_low = inversion_after_FS_x_aver - 1.96 * inversion_after_FS_x_std\n",
    "inversion_after_FS_x_high = inversion_after_FS_x_aver + 1.96 * inversion_after_FS_x_std\n",
    "\n",
    "inversion_after_FS_y_aver = np.mean(inversion_after_FS['y'].values)\n",
    "inversion_after_FS_y_std = np.std(inversion_after_FS['y'].values)\n",
    "inversion_after_FS_y_low = inversion_after_FS_y_aver - 1.96 * inversion_after_FS_y_std\n",
    "inversion_after_FS_y_high = inversion_after_FS_y_aver + 1.96 * inversion_after_FS_y_std\n",
    "\n",
    "print(\"After feature selection\")\n",
    "print(\"x interval: [\", inversion_after_FS_x_low, \",\" ,inversion_after_FS_x_high, \"]\")\n",
    "print(\"y interval: [\", inversion_after_FS_y_low, \",\" ,inversion_after_FS_y_high, \"]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 计算特征重要性\n",
    "source_feature_all = ['wave_rate1','aver1','median1','fft_shape_mean1','fft_mean1','SamEn1',\n",
    "                   'wave_rate2','aver2','median2','fft_shape_mean2','fft_mean2','SamEn2',\n",
    "                   'wave_rate3','aver3','median3','fft_shape_mean3','fft_mean3','SamEn3',\n",
    "                   'wave_rate4','aver4','median4','fft_shape_mean4','fft_mean4','SamEn4',]\n",
    "source_feature_all\n",
    "new_list = [source_feature_all[i] for i in range(len(source_feature_all)) if i not in id_delete]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_set = FontProperties(fname=r\"C:\\Windows\\Fonts\\simhei.ttf\", size=60)\n",
    "font_set2 = FontProperties(fname=r\"C:\\Windows\\Fonts\\simhei.ttf\", size=60)\n",
    "font_set3 = FontProperties(fname=r\"C:\\Windows\\Fonts\\Times.ttf\", size=18,weight=\"bold\")\n",
    "font_set4 = FontProperties(fname=r\"C:\\Windows\\Fonts\\Times.ttf\", size=16,weight=\"bold\")\n",
    "feature_importances0=multioutputregressor.estimators_[0].feature_importances_\n",
    "feature_importances1=multioutputregressor.estimators_[1].feature_importances_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ind = np.arange(len(new_list))\n",
    "width = 0.3\n",
    "rect1 = ax.barh(ind - width/2, feature_importances0,width,label='x',color='#130074',ec='black',lw=.5)\n",
    "rect1 = ax.barh(ind + width/2, feature_importances1,width,label='y',color='#CB181B',ec='black',lw=.5)\n",
    "plt.xticks(fontproperties='Times New Roman', size=16,weight=\"bold\")\n",
    "\n",
    "ax.set_yticks(ind + width / 2)\n",
    "ax.set_yticklabels(new_list,fontproperties=font_set3)\n",
    "font1 = {'family': 'Times New Roman',\n",
    "         'weight': 'bold',\n",
    "         'size': 22,\n",
    "         }\n",
    "plt.xlabel('Feature importance',font1)\n",
    "plt.yticks(fontproperties = 'Times New Roman', size = 16)\n",
    "text_font = {'size':'14','weight':'bold','color':'black'}\n",
    "ax.legend(prop={'family' : 'Times New Roman', 'size'   : 20, 'weight':'normal'},edgecolor='black',frameon=False)\n",
    "fig.savefig(\"./Reconstruction_results/Feature_importance_Group2.png\",dpi=900,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
